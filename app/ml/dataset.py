from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Tuple, List, Optional

import networkx as nx
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

from app.graph.features import degree_features, pagerank_features, motif_counts, merge_feature_maps


# ------------------------------------------------------------
# Unified label schema
# ------------------------------------------------------------

# We use a single binary target convention across datasets:
#   y = 1 -> suspicious/illicit node
#   y = 0 -> normal/lict node
LABEL_COL = "y"


@dataclass
class Dataset:
    X_train: pd.DataFrame
    X_val: pd.DataFrame
    y_train: pd.Series
    y_val: pd.Series


def graph_to_node_dataframe(G: nx.Graph, labels: dict | None = None) -> Tuple[pd.DataFrame, pd.Series | None]:
    feats = merge_feature_maps(degree_features(G), pagerank_features(G), motif_counts(G))
    df = pd.DataFrame.from_dict(feats, orient="index").fillna(0.0)
    y = None
    if labels:
        y = pd.Series({n: int(labels.get(n, 0)) for n in df.index}, name=LABEL_COL)
    return df, y


def train_val_split(X: pd.DataFrame, y: pd.Series | None, test_size: float = 0.2, seed: int = 42) -> Dataset:
    if y is None:
        # Unsupervised split shadow; assign zeros
        y = pd.Series(np.zeros(len(X), dtype=int), index=X.index, name=LABEL_COL)
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=seed, stratify=y)
    return Dataset(X_train, X_val, y_train, y_val)


# ------------------------------------------------------------
# Dataset loaders
# ------------------------------------------------------------


def load_synth(path: Path | str) -> Tuple[pd.DataFrame, pd.Series]:
    """Load the synthetic dataset generated by scripts/generate_synth.py.

    Input CSV columns expected: src, dst, amount, label
      - src, dst: string node identifiers (e.g., E123)
      - label: 1 if the transaction involves a risky entity, else 0

    Returns:
      - df_nodes: DataFrame indexed by node_id (no feature columns by default)
      - y: Series indexed by node_id with unified binary labels (y in {0,1})

    Note: As the generator labels transactions, node-level labels are induced as:
      y[node] = 1 if node participates in ANY transaction with label==1, else 0
    """
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(p)
    df = pd.read_csv(p)
    required = {"src", "dst", "label"}
    if not required.issubset(df.columns):
        raise ValueError(f"Missing required columns {required - set(df.columns)} in {p}")

    nodes = pd.Index(sorted(set(df["src"].astype(str)) | set(df["dst"].astype(str))), name="node_id")
    df_nodes = pd.DataFrame(index=nodes)

    suspicious_nodes = set(df.loc[df["label"].astype(int) == 1, "src"].astype(str)) | \
        set(df.loc[df["label"].astype(int) == 1, "dst"].astype(str))
    y = pd.Series([1 if n in suspicious_nodes else 0 for n in nodes], index=nodes, name=LABEL_COL, dtype=int)
    return df_nodes, y


def load_elliptic(path: Path | str) -> Tuple[pd.DataFrame, pd.Series]:
    """Placeholder for Elliptic dataset loader.

    Expected files/columns (documented, not fetched):
      - nodes CSV with columns: node_id, time_step, y (binary or mapped from {licit, illicit})
      - edges CSV with columns: src, dst (and optional amount/time)

    This function is a placeholder and intentionally not implemented to avoid
    fetching external data. Provide the path to preprocessed CSVs matching the
    above schema if you plan to implement it.
    """
    raise NotImplementedError(
        "load_elliptic is a placeholder. Expect nodes.csv with [node_id,time_step,y] and edges.csv with [src,dst]."
    )


# ------------------------------------------------------------
# Splitting utilities
# ------------------------------------------------------------


def make_splits(
    df: pd.DataFrame,
    stratify: bool = True,
    test_size: float = 0.2,
    val_size: float = 0.1,
    seed: int = 42,
) -> Tuple[pd.Index, pd.Index, pd.Index]:
    """Return train/val/test indices from a labeled DataFrame.

    - df must contain a binary label column 'y' (preferred) or 'label'.
    - test_size is the fraction for the held-out test split.
    - val_size is the fraction of the full dataset for validation (will be
      converted to a fraction of the remaining train after test split).
    """
    label_col = LABEL_COL if LABEL_COL in df.columns else ("label" if "label" in df.columns else None)
    if label_col is None:
        raise ValueError("DataFrame must include a 'y' or 'label' column for splitting")

    y = df[label_col].astype(int)
    idx = df.index

    strat = y if stratify else None
    try:
        idx_train_val, idx_test = train_test_split(idx, test_size=test_size, random_state=seed, stratify=strat)
    except ValueError:
        # Fallback when stratification is impossible due to tiny class counts
        idx_train_val, idx_test = train_test_split(idx, test_size=test_size, random_state=seed, stratify=None)

    # Adjust val_size relative to the remaining (train+val) pool
    val_rel = val_size / (1.0 - test_size)
    y_train_val = y.loc[idx_train_val]
    strat_tv = y_train_val if stratify else None
    try:
        idx_train, idx_val = train_test_split(idx_train_val, test_size=val_rel, random_state=seed, stratify=strat_tv)
    except ValueError:
        idx_train, idx_val = train_test_split(idx_train_val, test_size=val_rel, random_state=seed, stratify=None)

    return pd.Index(idx_train), pd.Index(idx_val), pd.Index(idx_test)
